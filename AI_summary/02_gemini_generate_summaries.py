"""
This script generates French summaries of text files using Google's Gemini 2.5 Flash model.

WORKFLOW:
1. Loads a prompt template from 'summary_prompt.md' file
2. Initializes Google Gemini client with API key or Application Default Credentials
3. Processes all .txt files from the TXT/ directory (generated by 01_extract_omeka_content.py)
4. For each text file, generates a French summary using the Gemini AI model
5. Saves generated summaries to the Summaries_FR_TXT/ directory
6. Provides progress tracking and comprehensive error handling

FEATURES:
- Supports both API key and Application Default Credentials authentication
- Uses external prompt template file for easy customization
- Configurable AI model parameters (temperature, etc.)
- Comprehensive error handling for API failures
- Progress tracking with detailed logging
- Skips empty files automatically

REQUIREMENTS:
- Environment variables: GEMINI_API_KEY or GOOGLE_APPLICATION_CREDENTIALS
- summary_prompt.md file containing the AI prompt template
- Text files in TXT/ directory from previous pipeline step
- Active internet connection for Gemini API access

OUTPUT:
- Summary files named {item_id}.txt in Summaries_FR_TXT/ directory
- Each file contains a French summary of the corresponding text content
"""

import os
# Use the new SDK structure
from google import genai
# Add the types import as per the new SDK structure
from google.genai import types
# Use the new SDK's error handling
from google.genai import errors
from tqdm import tqdm
from dotenv import load_dotenv
import logging

# Configure logging to track script execution, API calls, and errors
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load environment variables from .env file for API authentication
load_dotenv()

# AI model configuration
MODEL_NAME = 'gemini-2.5-flash'  # Google Gemini model version for summary generation

# Load the prompt template once at module level for efficiency
def load_prompt_template():
    """
    Loads the AI prompt template from the markdown file.
    
    This function reads the summary_prompt.md file which contains the instructions
    for the Gemini AI model on how to generate French summaries. The template
    includes a {text} placeholder that will be replaced with the actual content.
    
    Returns:
        str: The complete prompt template with {text} placeholder for content insertion
        
    Raises:
        FileNotFoundError: If the summary_prompt.md file is not found in the script directory
        Exception: If there's an error reading the file (permissions, encoding, etc.)
        
    Template Structure:
        The template should contain:
        - Instructions for generating French summaries
        - Format specifications for RAG compatibility
        - A {text} placeholder for content insertion
        
    File Location:
        Expects 'summary_prompt.md' to be in the same directory as this script.
        This design ensures the template is version-controlled with the code.
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_file = os.path.join(script_dir, 'summary_prompt.md')
    
    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()
            logging.debug(f"Successfully loaded prompt template from {prompt_file}")
            logging.debug(f"Template length: {len(content)} characters")
            return content
    except FileNotFoundError:
        logging.error(f"Prompt template file not found: {prompt_file}")
        logging.error("Please ensure 'summary_prompt.md' exists in the same directory as this script.")
        raise FileNotFoundError(f"Required prompt template file not found: {prompt_file}. Please ensure 'summary_prompt.md' exists in the same directory as this script.")
    except Exception as e:
        logging.error(f"Error reading prompt template: {e}")
        raise RuntimeError(f"Failed to load prompt template from '{prompt_file}': {e}")

# Load the prompt template once at startup for efficiency
PROMPT_TEMPLATE = load_prompt_template()


def initialize_gemini_client():
    """
    Initializes the Google Generative AI Client using the new SDK with dual authentication support.
    
    This function implements a fallback authentication strategy:
    1. First attempts to use Application Default Credentials (ADC) if configured
    2. Falls back to API key authentication if ADC is not available
    3. Provides detailed logging for troubleshooting authentication issues
    
    Authentication Methods:
        - ADC: Uses GOOGLE_APPLICATION_CREDENTIALS environment variable
        - API Key: Uses GEMINI_API_KEY environment variable
        
    Returns:
        genai.Client: Initialized and authenticated Gemini client instance
        
    Raises:
        ValueError: If neither authentication method succeeds
        errors.APIError: If there's an API configuration issue
        
    Environment Variables:
        - GOOGLE_APPLICATION_CREDENTIALS: Path to service account JSON file (for ADC)
        - GEMINI_API_KEY: Direct API key for authentication
    """
    google_api_key = None
    client = None
    auth_method = "Unknown"

    # Authentication Method 1: Try Application Default Credentials (ADC)
    credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
    if credentials_path:
        if os.path.exists(credentials_path):
            logging.info("Attempting client initialization using Google Application Default Credentials (ADC) from: %s", credentials_path)
            try:
                # The new SDK picks up ADC automatically when no api_key is provided
                client = genai.Client()
                # Optional: Test connectivity (uncomment if needed)
                # client.models.list()
                auth_method = "ADC"
                logging.info("Gemini Client initialized successfully using ADC.")
            except Exception as e:
                logging.warning(f"ADC initialization failed: {e}. Falling back to API key.")
                client = None  # Ensure client is None if ADC fails
        else:
            logging.warning(
                "GOOGLE_APPLICATION_CREDENTIALS is set, but file not found at: %s. "
                "Falling back to API key.", credentials_path
            )

    # Authentication Method 2: Try API Key if ADC failed or wasn't configured
    if client is None:
        google_api_key = os.getenv('GEMINI_API_KEY')
        if google_api_key:
            logging.info("Attempting client initialization using GEMINI_API_KEY from environment.")
            try:
                # Pass the API key directly to the client constructor
                client = genai.Client(api_key=google_api_key)
                # Optional: Test connectivity (uncomment if needed)
                # client.models.list()
                auth_method = "API Key"
                logging.info("Gemini Client initialized successfully using API Key.")
            except Exception as e:
                logging.error(f"Failed to initialize Gemini Client with API key: {e}")
                raise ValueError("Failed to initialize Gemini Client with the provided GEMINI_API_KEY.") from e
        else:
            # Both authentication methods failed
            raise ValueError(
                "Authentication failed. Gemini Client could not be initialized. "
                "Please configure ADC (GOOGLE_APPLICATION_CREDENTIALS) OR "
                "set the GEMINI_API_KEY in your environment/.env file."
            )

    # Return the successfully initialized client
    if client:
        logging.info(f"Using model: {MODEL_NAME} with {auth_method} authentication")
        return client
    else:
        # This should not be reached due to prior exception handling
        raise RuntimeError("Failed to initialize Gemini Client - unexpected error.")


def generate_summary_with_gemini(client, text):
    """
    Generate a French summary of the provided text using the Gemini AI model.
    
    This function uses the pre-loaded prompt template and configured AI model
    to generate concise French summaries optimized for RAG (Retrieval-Augmented Generation) systems.
    
    Args:
        client (genai.Client): Initialized and authenticated Gemini client instance
        text (str): The input text content to summarize (typically OCR text)
        
    Returns:
        str: Generated French summary text, or None if generation fails
        
    AI Configuration:
        - Model: Gemini 2.5 Flash (configured in MODEL_NAME)
        - Temperature: 0.2 (low for factual, consistent summaries)
        - Prompt: Loaded from summary_prompt.md template
        
    Error Handling:
        - API errors are logged with details for troubleshooting
        - Network issues are handled gracefully
        - Invalid responses are detected and reported
    """
    # Insert the text content into the prompt template
    prompt = PROMPT_TEMPLATE.format(text=text)

    try:
        # Configure AI generation parameters for factual summary generation
        gen_config = types.GenerateContentConfig(
            temperature=0.2  # Low temperature for consistent, factual summaries
            # Additional parameters can be added here:
            # top_p=0.95, top_k=20, max_output_tokens=500, etc.
        )

        # Generate content using the Gemini API
        response = client.models.generate_content(
            model=MODEL_NAME,  # Use configured model
            contents=prompt,   # Formatted prompt with text content
            config=gen_config  # AI generation configuration
        )
        
        # Validate response and extract summary text
        if response and hasattr(response, 'text'):
            # Clean the response text (remove potential markdown formatting)
            summary = response.text.strip().replace('*', '')
            logging.debug(f"Generated summary length: {len(summary)} characters")
            return summary
        else:
            logging.error("Received an unexpected response format from Gemini API.")
            # For debugging: uncomment to log full response (be careful with sensitive data)
            # logging.debug(f"Full Gemini response: {response}")
            return None
            
    except errors.APIError as e:
        logging.error(f"Gemini API call failed: {e}")
        # Log additional error details if available
        if hasattr(e, 'code'):
            logging.error(f"API Error Code: {e.code}")
        if hasattr(e, 'message'):
            logging.error(f"API Error Message: {e.message}")
        return None
    except Exception as e:
        # Catch any other unexpected errors during generation
        logging.error(f"Unexpected error during summary generation: {e}")
        return None

def process_file(client, input_file_path, output_file_path):
    """
    Process a single text file to generate and save its French summary.
    
    This function reads a text file, generates a French summary using the Gemini AI model,
    and saves the result to the specified output path. It handles various error conditions
    gracefully and provides detailed logging for troubleshooting.
    
    Args:
        client (genai.Client): Initialized and authenticated Gemini client instance
        input_file_path (str): Full path to the input text file to process
        output_file_path (str): Full path where the summary file will be saved
        
    Process Flow:
        1. Reads the input text file with UTF-8 encoding
        2. Validates that the file contains content (skips empty files)
        3. Generates a French summary using the Gemini AI model
        4. Saves the summary to the output file if generation succeeds
        5. Logs all operations for monitoring and debugging
        
    Error Handling:
        - FileNotFoundError: Logged when input file doesn't exist
        - IOError: Handles file permission and encoding issues
        - Empty files: Skipped with warning log
        - AI generation failures: Logged with error details
    """
    try:
        # Read the input text file with UTF-8 encoding
        with open(input_file_path, 'r', encoding='utf-8') as infile:
            original_text = infile.read()

        # Validate file content - skip empty files
        if not original_text.strip():
            logging.warning(f"Input file is empty, skipping: {input_file_path}")
            return  # Skip empty files to avoid unnecessary API calls

        # Log the processing start for monitoring
        logging.info(f"Generating summary for: {os.path.basename(input_file_path)}")
        
        # Generate the French summary using the Gemini AI model
        summary_text = generate_summary_with_gemini(client, original_text)

        # Save the summary if generation was successful
        if summary_text:
            with open(output_file_path, 'w', encoding='utf-8') as outfile:
                outfile.write(summary_text)
            logging.info(f"Summary saved to: {os.path.basename(output_file_path)}")
            logging.debug(f"Summary length: {len(summary_text)} characters")
        else:
            logging.error(f"Failed to generate summary for: {os.path.basename(input_file_path)}")

    except FileNotFoundError:
        logging.error(f"Input file not found: {input_file_path}")
    except IOError as e:
        logging.error(f"Error processing file {input_file_path}: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred processing file {input_file_path}: {e}")

def process_txt_files(client, input_dir, output_dir):
    """
    Process all text files in a directory to generate French summaries.
    
    This function orchestrates the batch processing of multiple text files,
    automatically discovering all .txt files in the input directory and
    generating corresponding summary files in the output directory.
    
    Args:
        client (genai.Client): Initialized and authenticated Gemini client instance
        input_dir (str): Directory path containing input .txt files to process
        output_dir (str): Directory path where summary .txt files will be saved
        
    Directory Structure:
        - Input directory: Contains original .txt files from OCR processing
        - Output directory: Will contain corresponding summary files with same names
        - Creates output directory if it doesn't exist
        
    Process Flow:
        1. Validates input directory exists
        2. Creates output directory if necessary
        3. Discovers all .txt files in input directory
        4. Processes each file individually with progress tracking
        5. Provides comprehensive logging for monitoring
        
    Error Handling:
        - Missing input directory: Logged and function returns early
        - Output directory creation failures: Logged with specific error
        - No .txt files found: Warning logged
        - Individual file processing errors: Handled in process_file function
    """
    # Validate input directory exists
    if not os.path.exists(input_dir):
        logging.error(f"Input directory not found: {input_dir}")
        return

    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
            logging.info(f"Created output directory: {output_dir}")
        except OSError as e:
            logging.error(f"Failed to create output directory {output_dir}: {e}")
            return

    # Discover all .txt files in the input directory
    txt_files = [f for f in os.listdir(input_dir) if f.endswith('.txt')]

    # Check if any .txt files were found
    if not txt_files:
        logging.warning(f"No .txt files found in input directory: {input_dir}")
        return

    # Log the batch processing start
    logging.info(f"Found {len(txt_files)} .txt files to process")
    logging.info(f"Processing files from: {input_dir}")
    logging.info(f"Saving summaries to: {output_dir}")

    # Process each file with progress tracking
    for txt_file in tqdm(txt_files, desc="Generating Summaries"):
        input_file_path = os.path.join(input_dir, txt_file)
        output_file_path = os.path.join(output_dir, txt_file)  # Keep the same filename for the summary
        process_file(client, input_file_path, output_file_path)
    
    # Log completion summary
    logging.info(f"Batch processing completed for {len(txt_files)} files")

def main():
    """
    Main execution function that orchestrates the French summary generation pipeline.
    
    This function serves as the entry point for the summary generation process,
    coordinating directory setup, client initialization, and batch file processing.
    It implements comprehensive error handling for all potential failure points.
    
    Pipeline Overview:
        1. Determines script directory for relative path resolution
        2. Sets up input and output directory paths
        3. Initializes the Gemini AI client with authentication
        4. Processes all .txt files in the input directory
        5. Generates French summaries for each file
        6. Saves results to the output directory
        
    Directory Structure:
        - Input: ./TXT/ (contains OCR text files from previous pipeline step)
        - Output: ./Summaries_FR_TXT/ (will contain generated French summaries)
        
    Error Handling:
        - ValueError: Configuration or authentication issues
        - FileNotFoundError: Missing prompt template file
        - APIError: Gemini API communication problems
        - Exception: Unexpected errors during execution
        
    Dependencies:
        - Requires summary_prompt.md in the same directory
        - Requires valid Gemini API credentials (ADC or API key)
        - Requires input .txt files from OCR processing
    """
    try:
        # Get the directory of the current script for relative path resolution
        script_dir = os.path.dirname(os.path.abspath(__file__))

        # Set up input and output directories relative to the script directory
        input_dir = os.path.join(script_dir, 'TXT')  # Directory containing the input .txt files
        output_dir = os.path.join(script_dir, 'Summaries_FR_TXT')  # Directory for French summaries

        # Log the pipeline start with directory information
        logging.info("Starting French summary generation pipeline")
        logging.info(f"Input directory: {input_dir}")
        logging.info(f"Output directory: {output_dir}")

        # Initialize Gemini client with authentication validation
        gemini_client = initialize_gemini_client()

        # Process all text files in the input directory
        process_txt_files(gemini_client, input_dir, output_dir)

        # Log successful completion
        logging.info("French summary generation pipeline completed successfully")

    except ValueError as e:
        # Handle configuration or authentication errors
        logging.error(f"Configuration or initialization error: {e}")
        logging.error("Please check your authentication credentials and try again")
    except FileNotFoundError as e:
        # Handle missing prompt template file
        logging.error(f"Prompt template error: {e}")
        logging.error("Please ensure 'summary_prompt.md' exists in the script directory")
    except errors.APIError as e:
        # Handle Gemini API communication errors
        logging.error(f"Gemini API error during setup: {e}")
        logging.error("Please check your API credentials and network connection")
    except Exception as e:
        # Handle any unexpected errors
        logging.error(f"An unexpected error occurred during execution: {e}")
        logging.error("Please check the logs for more details")

if __name__ == "__main__":
    main()
